{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:33.876488100Z",
     "start_time": "2024-05-17T09:11:33.472718100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from math import sqrt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51c033ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Embeddings Shape: (165, 467)\n",
      "Validation Embeddings Shape: (42, 467)\n",
      "Text Embedding Shape: (1, 467)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews_mixed.csv')\n",
    "\n",
    "# Define lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.lower().split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to the dataset\n",
    "df['Text_Lemmatized'] = df['Text'].apply(lemmatize_text)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "training_input, validation_input, training_output, validation_output = train_test_split(df['Text_Lemmatized'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text data to feature vectors\n",
    "training_embeddings = vectorizer.fit_transform(training_input).toarray()\n",
    "validation_embeddings = vectorizer.transform(validation_input).toarray()\n",
    "\n",
    "# Example text for prediction\n",
    "text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "text_lemmatized = [lemmatize_text(t) for t in text]\n",
    "text_embedding = vectorizer.transform(text_lemmatized).toarray()\n",
    "\n",
    "print(\"Training Embeddings Shape:\", training_embeddings.shape)\n",
    "print(\"Validation Embeddings Shape:\", validation_embeddings.shape)\n",
    "print(\"Text Embedding Shape:\", text_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc42aac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Embeddings Shape: (165, 768)\n",
      "Validation Embeddings Shape: (42, 768)\n",
      "Example Embedding Shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "# pip install torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews_mixed.csv')\n",
    "\n",
    "# Define function to preprocess text (lemmatization + stop words removal)\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.lower().split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['Text_Preprocessed'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "training_input, validation_input, training_output, validation_output = train_test_split(\n",
    "    df['Text_Preprocessed'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define function to get BERT embeddings\n",
    "def get_bert_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Get BERT embeddings for training and validation sets\n",
    "training_embeddings = get_bert_embeddings(training_input.tolist())\n",
    "validation_embeddings = get_bert_embeddings(validation_input.tolist())\n",
    "\n",
    "# Example text for prediction\n",
    "example_text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "example_embeddings = get_bert_embeddings([preprocess_text(t) for t in example_text])\n",
    "\n",
    "print(\"Training Embeddings Shape:\", training_embeddings.shape)\n",
    "print(\"Validation Embeddings Shape:\", validation_embeddings.shape)\n",
    "print(\"Example Embedding Shape:\", example_embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a60eed7cf54225f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.005834200Z",
     "start_time": "2024-05-17T09:11:33.489240500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Embeddings Shape: (165, 446)\n",
      "Validation Embeddings Shape: (98, 446)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('reviews_mixed.csv')\n",
    "\n",
    "def get_training_and_validation_datas(df: pd.DataFrame, training_size = 0.8):\n",
    "    data_size = df.shape[0]\n",
    "    indexes = [i for i in range(data_size)]\n",
    "    training_index = np.random.choice(indexes,int(data_size*training_size))\n",
    "    validation_index = [i for i in range(data_size) if not i in training_index]\n",
    "    training_input = [df['Text'].iloc[index] for index in training_index]\n",
    "    training_output = [df['Sentiment'].iloc[index] for index in training_index]\n",
    "    validation_input = [df['Text'].iloc[index] for index in validation_index]\n",
    "    validation_output = [df['Sentiment'].iloc[index] for index in validation_index]\n",
    "    return training_input, training_output, validation_input, validation_output\n",
    "\n",
    "training_input,training_output,validation_input, validation_output = get_training_and_validation_datas(df)\n",
    "\n",
    "#Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#TF-IDF\n",
    "# vectorizer = TfidfVectorizer(max_features=50)\n",
    "\n",
    "text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "training_embeddings = vectorizer.fit_transform(training_input).toarray()\n",
    "validation_embeddings = vectorizer.transform(validation_input).toarray()\n",
    "\n",
    "\n",
    "print(\"Training Embeddings Shape:\", training_embeddings.shape)\n",
    "print(\"Validation Embeddings Shape:\", validation_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b214d1-dcd5-46a4-9026-ecd7df77c607",
   "metadata": {},
   "source": [
    "## KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "68c2099b75fbafe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.090879700Z",
     "start_time": "2024-05-17T09:11:33.551845200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.28571428571429%\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "kmeans.fit(training_embeddings)\n",
    "\n",
    "label_names = [name for name in set(training_output)]\n",
    "validation_indexes = kmeans.predict(validation_embeddings)\n",
    "computed_outputs = [label_names[value] for value in validation_indexes]\n",
    "\n",
    "accuracy = accuracy_score(validation_output, computed_outputs)\n",
    "print(f'Accuracy: {accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e4fa32de58e4444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.157566800Z",
     "start_time": "2024-05-17T09:11:33.982454700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: negative\n"
     ]
    }
   ],
   "source": [
    "text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "input =  vectorizer.transform(text).toarray()\n",
    "label = kmeans.predict(input)\n",
    "print('Label:', label_names[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c1791a723f5bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.174383600Z",
     "start_time": "2024-05-17T09:11:34.025203500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.8936170212766%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglomerative = AgglomerativeClustering(n_clusters=2)\n",
    "agglomerative.fit(training_embeddings)\n",
    "\n",
    "label_names = [name for name in set(training_output)]\n",
    "validation_indexes = agglomerative.fit_predict(validation_embeddings)\n",
    "computed_outputs = [label_names[value] for value in validation_indexes]\n",
    "\n",
    "accuracy = accuracy_score(validation_output, computed_outputs)\n",
    "print(f'Accuracy: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ce0e129a289b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.693301800Z",
     "start_time": "2024-05-17T09:11:34.113645300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.42857142857143%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(training_embeddings)\n",
    "\n",
    "label_names = [name for name in set(training_output)]\n",
    "validation_indexes = gmm.predict(validation_embeddings)\n",
    "computed_outputs = [label_names[value] for value in validation_indexes]\n",
    "\n",
    "accuracy = accuracy_score(validation_output, computed_outputs)\n",
    "print(f'Accuracy: {accuracy * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
